---
footer: "[üîó posit.io/arrow](https://posit-conf-2023.github.io/arrow)"
logo: "images/logo.png"
execute:
  echo: true
format:
  revealjs: 
    theme: default
engine: knitr
---

```{r}
#| label: setup-eng
#| echo: false
library(arrow)
library(dplyr)

seattle_parquet <- here::here("data/seattle-library-checkouts-parquet")
seattle_parquet_part <- here::here("data/seattle-library-checkouts")
```

# Data Engineering with Arrow {#data-eng-storage}

## Data Engineering

<br>

![](images/data-engineering.png)

<br>

::: {style="font-size: 70%;"}
<https://en.wikipedia.org/wiki/Data_engineering>
:::

## .NORM Files

![](images/norm_normal_file_format_2x.png){.absolute top="0" left="400"}

<br>

::: {style="font-size: 70%;"}
<https://xkcd.com/2116/>
:::

## Formats

![](images/big-data-formats-luminousman.png){.absolute top="0" left="250"}

::: {style="font-size: 60%; margin-top: 550px;"}
<https://luminousmen.com/post/big-data-file-formats>
:::

::: notes
There are lots of big data/columnar formats (not all supported by Arrow we are only covering Parquet and CSV --- CSV is still a big player in the file format world, so we will learn how to work with CSVs with Arrow
:::

## Arrow & File Formats

![](images/arrow-read-write-updated.png)

## Poll: Formats

<br>

Which file formats do you use most often?

-   CSV (.csv)
-   MS Excel (.xls and .xlsx)
-   Parquet (.parquet)
-   Something else

## Seattle<br>Checkouts<br>Big CSV

![](images/seattle-checkouts.png){.absolute top="0" left="300"}

::: notes
<https://data.seattle.gov/Community/Checkouts-by-Title/tmmm-ytt6>
:::

## arrow::open_dataset() with a CSV

```{r}
#| label: open-seattle-data
library(arrow)
library(dplyr)

seattle_csv <- open_dataset(
  sources = here::here("data/seattle-library-checkouts.csv"), 
  format = "csv"
)

seattle_csv
```

## üëÄ Glimpse

```{r}
#| label: seattle-glimpse
seattle_csv |> glimpse()
```

## Parsing the Metadata

```{r}
#| label: seattle-schema
seattle_csv$schema
```

## Parsing the Metadata

<br>

Arrow scans üëÄ a few thousand rows of the file(s) to impute or "guess" the data types

::: {style="font-size: 80%; margin-top: 200px;"}
üìö arrow vs readr blog post: <https://thisisnic.github.io/2022/11/21/type-inference-in-readr-and-arrow/>
:::

## Parsers Are Not Always Right

```{r}
#| label: seattle-schema-again
seattle_csv$schema
```

::: notes
International Standard Book Number (ISBN) is a 13-digit number that uniquely identifies books and book-like products published internationally.

Data Dictionaries, metadata in data catalogues should provide this info.
:::

## Arrow Data Types

Arrow has a rich data type system, including direct analogs of many R data types

-   `<dbl>` == `<double>`
-   `<chr>` == `<string>` or `<utf8>`
-   `<int>` == `<int32>`

<br>

<https://arrow.apache.org/docs/r/articles/data_types.html>

## Arrow's schema()

<br>

```{r}
#| label: seattle-schema-code
seattle_csv$schema$code() 
```

## Let's Control the Schema

```{r}
#| label: seattle-schema-control
#| code-line-numbers: "|13"
seattle_csv <- open_dataset(
  sources = here::here("data/seattle-library-checkouts.csv"),
  format = "csv",
  skip = 1,
  schema = schema(
    UsageClass = utf8(),
    CheckoutType = utf8(),
    MaterialType = utf8(),
    CheckoutYear = int64(),
    CheckoutMonth = int64(),
    Checkouts = int64(),
    Title = utf8(),
    ISBN = string(), #utf8()
    Creator = utf8(),
    Subjects = utf8(),
    Publisher = utf8(),
    PublicationYear = utf8()
  )
)
seattle_csv
```

## Your Turn

1.  The first few thousand rows of `ISBN` are blank in the Seattle Checkouts CSV file. Read in the Seattle Checkouts CSV file with `open_dataset()` and ensure the correct data type for `ISBN` is `<string>` instead of the `<null>` interpreted by Arrow.
2.  Once you have a `Dataset` object with the metadata you are after, count the number of `Checkouts` by `CheckoutYear` and arrange the result by `CheckoutYear`.

‚û°Ô∏è [Data Storage Engineering Exercises Page](3_data_engineering-exercises.html)

## 9GB CSV file + arrow + dplyr

```{r}
#| label: seattle-dplyr
seattle_csv |>
  group_by(CheckoutYear) |>
  summarise(sum(Checkouts)) |>
  collect()
```

## 9GB CSV file + arrow + dplyr

```{r}
#| label: seattle-dplyr-timed
#| code-line-numbers: "2,7"
library(tictoc)
tic()
seattle_csv |>
  group_by(CheckoutYear) |>
  summarise(sum(Checkouts)) |>
  collect()
toc()
```

42 million rows -- not bad, but could be faster....

## File Format: Apache Parquet

![](images/apache-parquet.png){.absolute top="100" left="200" width="700"}

::: {style="font-size: 60%; margin-top: 450px;"}
<https://parquet.apache.org/>
:::

## Parquet

-   usually smaller than equivalent CSV file
-   rich type system & stores the data type along with the data
-   "column-oriented" == better performance over CSV's row-by-row
-   "row-chunked" == work on different parts of the file at the same time or skip some chunks all together

::: notes
-   efficient encodings to keep file size down, and supports file compression, less data to move from disk to memory
-   CSV has no info about data types, inferred by each parser
:::

## Parquet Files: "row-chunked"

![](images/parquet-chunking.png)

## Parquet Files: "row-chunked & column-oriented"

![](images/parquet-columnar.png)

## Writing to Parquet

```{r}
#| label: seattle-write-parquet-single
#| eval: false
seattle_parquet <- here::here("data/seattle-library-checkouts-parquet")

seattle_csv |>
  write_dataset(path = seattle_parquet,
                format = "parquet")
```

## Storage: Parquet vs CSV

```{r}
#| label: seattle-single-parquet-size
file <- list.files(seattle_parquet)
file.size(file.path(seattle_parquet, file)) / 10**9
```

<br>

Parquet about half the size of the CSV file on-disk üíæ

## Your Turn

1.  Re-run the query counting the number of `Checkouts` by `CheckoutYear` and arranging the result by `CheckoutYear`, this time using the Seattle Checkout data saved to disk as a single, Parquet file. Did you notice a difference in compute time?

‚û°Ô∏è [Data Storage Engineering Exercises Page](3_data_engineering-exercises.html)

## 4.5GB Parquet file + arrow + dplyr

```{r}
#| label: seattle-single-parquet-dplyr-timed
tic()
open_dataset(seattle_parquet, 
             format = "parquet") |>
  group_by(CheckoutYear) |>
  summarise(sum(Checkouts)) |>
  arrange(CheckoutYear) |> 
  collect() 
toc()
```

42 million rows -- much better! But could be *even* faster....

## File Storage:<br>Partitioning

<br>

::: columns
::: {.column width="50%"}
Dividing data into smaller pieces, making it more easily accessible and manageable
:::

::: {.column width="50%"}
![](images/partitions.png){.absolute top="0"}
:::
:::

::: notes
also called multi-files or sometimes shards
:::

## Poll: Partitioning?

Have you partitioned your data or used partitioned data before today?

-   Yes
-   No
-   Not sure, the data engineers sort that out!

## Art & Science of Partitioning

<br>

-   avoid files \< 20MB and \> 2GB
-   avoid \> 10,000 files (ü§Ø)
-   partition on variables used in `filter()`

::: notes
-   guidelines not rules, results vary
-   experiment
-   arrow suggests avoid files smaller than 20MB and larger than 2GB
-   avoid partitions that produce more than 10,000 files
-   partition by variables that you filter by, allows arrow to only read relevant files
:::

## Rewriting the Data Again

```{r}
#| label: seattle-write-partitioned
#| eval: false
seattle_parquet_part <- here::here("data/seattle-library-checkouts")

seattle_csv |>
  group_by(CheckoutYear) |>
  write_dataset(path = seattle_parquet_part,
                format = "parquet")
```

## What Did We "Engineer"?

```{r}
#| label: seattle-partitioned-sizes
seattle_parquet_part <- here::here("data/seattle-library-checkouts")

sizes <- tibble(
  files = list.files(seattle_parquet_part, recursive = TRUE),
  size_GB = file.size(file.path(seattle_parquet_part, files)) / 10**9
)

sizes
```

## 4.5GB partitioned Parquet files + arrow + dplyr

```{r}
#| label: seattle-partitioned-dplyr-timed
seattle_parquet_part <- here::here("data/seattle-library-checkouts")

tic()
open_dataset(seattle_parquet_part,
             format = "parquet") |>
  group_by(CheckoutYear) |>
  summarise(sum(Checkouts)) |>
  collect() 
toc()
```

<br>

42 million rows -- not too shabby!

## Your Turn

1.  Let's write the Seattle Checkout CSV data to a multi-file dataset just one more time! This time, write the data partitioned by `CheckoutType` as Parquet files.

2.  Now compare the compute time between our Parquet data partitioned by `CheckoutYear` and our Parquet data partitioned by `ChekcoutType` with a query of the total number of checkouts in September of 2019. Did you find a difference in compute time?

‚û°Ô∏è [Data Storage Engineering Exercises Page](3_data_engineering-exercises.html)

## Partition Design

::: columns
::: {.column width="50%"}
-   Partitioning on variables commonly used in `filter()` often faster
-   Number of partitions also important (Arrow reads the metadata of each file)
:::

::: {.column width="50%"}
![](images/partitions.png){.absolute top="0"}
:::
:::

## Performance Review: Single CSV

How long does it take to calculate the number of books checked out in each month of 2021?

<br>

```{r}
#| label: seattle-single-csv-dplyr-timed
tic()
open_dataset(
  sources = here::here("data/seattle-library-checkouts.csv"), 
  format = "csv"
) |> 
  filter(CheckoutYear == 2021, MaterialType == "BOOK") |>
  group_by(CheckoutMonth) |>
  summarize(TotalCheckouts = sum(Checkouts)) |>
  arrange(desc(CheckoutMonth)) |>
  collect()
toc()
```

## Performance Review: Partitioned Parquet

How long does it take to calculate the number of books checked out in each month of 2021?

<br>

```{r}
#| label: seattle-parquet-partitioned-dplyr-timed
tic()
open_dataset(here::here("data/seattle-library-checkouts"),
             format = "parquet") |> 
  filter(CheckoutYear == 2021, MaterialType == "BOOK") |>
  group_by(CheckoutMonth) |>
  summarize(TotalCheckouts = sum(Checkouts)) |>
  arrange(desc(CheckoutMonth)) |>
  collect() 
toc()
```

## Engineering Data Tips for Improved Storage & Performance

<br>

-   consider "column-oriented" file formats like Parquet
-   consider partitioning, experiment to get an appropriate partition design üóÇÔ∏è
-   watch your schemas üëÄ

## R for Data Science (2e)

::: columns
::: {.column width="50%"}
![](images/r4ds-cover.jpg){.absolute top="100" width="400"}
:::

::: {.column width="50%"}
<br>

[Chapter 23: Arrow](https://r4ds.hadley.nz/arrow.html)

<br>

<https://r4ds.hadley.nz/>
:::
:::
