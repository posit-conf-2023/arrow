{
  "hash": "f8a60b09320bad324f0afe998914f147",
  "result": {
    "markdown": "---\nfooter: \"[üîó posit.io/arrow](https://posit-conf-2023.github.io/arrow)\"\nlogo: \"images/logo.png\"\nexecute:\n  echo: true\nformat:\n  revealjs: \n    theme: default\nengine: knitr\n---\n\n::: {.cell}\n\n:::\n\n\n# Data Engineering with Arrow {#data-eng-storage}\n\n## Data Engineering\n\n<br>\n\n![](images/data-engineering.png)\n\n<br>\n\n::: {style=\"font-size: 70%;\"}\n<https://en.wikipedia.org/wiki/Data_engineering>\n:::\n\n## .NORM Files\n\n![](images/norm_normal_file_format_2x.png){.absolute top=\"0\" left=\"400\"}\n\n<br>\n\n::: {style=\"font-size: 70%;\"}\n<https://xkcd.com/2116/>\n:::\n\n## Formats\n\n![](images/big-data-formats-luminousman.png){.absolute top=\"0\" left=\"250\"}\n\n::: {style=\"font-size: 60%; margin-top: 550px;\"}\n<https://luminousmen.com/post/big-data-file-formats>\n:::\n\n::: notes\nThere are lots of big data/columnar formats (not all supported by Arrow we are only covering Parquet and CSV --- CSV is still a big player in the file format world, so we will learn how to work with CSVs with Arrow\n:::\n\n## Arrow & File Formats\n\n![](images/arrow-read-write-updated.png)\n\n## Poll: Formats\n\n<br>\n\nWhich file formats do you use most often?\n\n-   CSV (.csv)\n-   MS Excel (.xls and .xlsx)\n-   Parquet (.parquet)\n-   Something else\n\n## Seattle<br>Checkouts<br>Big CSV\n\n![](images/seattle-checkouts.png){.absolute top=\"0\" left=\"300\"}\n\n::: notes\n<https://data.seattle.gov/Community/Checkouts-by-Title/tmmm-ytt6>\n:::\n\n## Download the 9GB CSV file\n\n<br>\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncurl::multi_download(\n  \"https://r4ds.s3.us-west-2.amazonaws.com/seattle-library-checkouts.csv\",\n  here::here(\"data/seattle-library-checkouts.csv\"),\n  resume = TRUE\n)\n```\n:::\n\n\n## arrow::open_dataset() with a CSV\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(arrow)\nlibrary(dplyr)\n\nseattle_csv <- open_dataset(\n  sources = here::here(\"data/seattle-library-checkouts.csv\"), \n  format = \"csv\"\n)\n\nseattle_csv\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFileSystemDataset with 1 csv file\nUsageClass: string\nCheckoutType: string\nMaterialType: string\nCheckoutYear: int64\nCheckoutMonth: int64\nCheckouts: int64\nTitle: string\nISBN: null\nCreator: string\nSubjects: string\nPublisher: string\nPublicationYear: string\n```\n:::\n:::\n\n\n## üëÄ Glimpse\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_csv |> glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFileSystemDataset with 1 csv file\n41,389,465 rows x 12 columns\n$ UsageClass      <string> \"Physical\", \"Physical\", \"Digital\", \"Physical\", \"Physi‚Ä¶\n$ CheckoutType    <string> \"Horizon\", \"Horizon\", \"OverDrive\", \"Horizon\", \"Horizo‚Ä¶\n$ MaterialType    <string> \"BOOK\", \"BOOK\", \"EBOOK\", \"BOOK\", \"SOUNDDISC\", \"BOOK\",‚Ä¶\n$ CheckoutYear     <int64> 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016,‚Ä¶\n$ CheckoutMonth    <int64> 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,‚Ä¶\n$ Checkouts        <int64> 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 2, 3, 2, 1, 3, 2, 3,‚Ä¶\n$ Title           <string> \"Super rich : a guide to having it all / Russell Simm‚Ä¶\n$ ISBN              <null> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ Creator         <string> \"Simmons, Russell\", \"Barclay, James, 1965-\", \"Tim Par‚Ä¶\n$ Subjects        <string> \"Self realization, Conduct of life, Attitude Psycholo‚Ä¶\n$ Publisher       <string> \"Gotham Books,\", \"Pyr,\", \"Random House, Inc.\", \"Dial ‚Ä¶\n$ PublicationYear <string> \"c2011.\", \"2010.\", \"2015\", \"2005.\", \"c2004.\", \"c2005.‚Ä¶\n```\n:::\n:::\n\n\n## Parsing the Metadata\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_csv$schema\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSchema\nUsageClass: string\nCheckoutType: string\nMaterialType: string\nCheckoutYear: int64\nCheckoutMonth: int64\nCheckouts: int64\nTitle: string\nISBN: null\nCreator: string\nSubjects: string\nPublisher: string\nPublicationYear: string\n```\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n## Parsing the Metadata\n\n<br>\n\nArrow scans üëÄ a few thousand rows of the file(s) to impute or \"guess\" the data types\n\n::: {style=\"font-size: 80%; margin-top: 200px;\"}\nüìö arrow vs readr blog post: <https://thisisnic.github.io/2022/11/21/type-inference-in-readr-and-arrow/>\n:::\n\n## Parsers Are Not Always Right\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_csv$schema\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSchema\nUsageClass: string\nCheckoutType: string\nMaterialType: string\nCheckoutYear: int64\nCheckoutMonth: int64\nCheckouts: int64\nTitle: string\nISBN: null\nCreator: string\nSubjects: string\nPublisher: string\nPublicationYear: string\n```\n:::\n:::\n\n\n::: notes\nInternational Standard Book Number (ISBN) is a 13-digit number that uniquely identifies books and book-like products published internationally.\n\nData Dictionaries, metadata in data catalogues should provide this info.\n:::\n\n## Arrow Data Types\n\nArrow has a rich data type system, including direct analogs of many R data types\n\n-   `<dbl>` == `<double>`\n-   `<chr>` == `<string>` or `<utf8>`\n-   `<int>` == `<int32>`\n\n<br>\n\n<https://arrow.apache.org/docs/r/articles/data_types.html>\n\n## Arrow's schema()\n\n<br>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_csv$schema$code() \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nschema(UsageClass = utf8(), CheckoutType = utf8(), MaterialType = utf8(), \n    CheckoutYear = int64(), CheckoutMonth = int64(), Checkouts = int64(), \n    Title = utf8(), ISBN = null(), Creator = utf8(), Subjects = utf8(), \n    Publisher = utf8(), PublicationYear = utf8())\n```\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n## Let's Control the Schema\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|13\"}\nseattle_csv <- open_dataset(\n  sources = here::here(\"data/seattle-library-checkouts.csv\"),\n  format = \"csv\",\n  skip = 1,\n  schema = schema(\n    UsageClass = utf8(),\n    CheckoutType = utf8(),\n    MaterialType = utf8(),\n    CheckoutYear = int64(),\n    CheckoutMonth = int64(),\n    Checkouts = int64(),\n    Title = utf8(),\n    ISBN = string(), #utf8()\n    Creator = utf8(),\n    Subjects = utf8(),\n    Publisher = utf8(),\n    PublicationYear = utf8()\n  )\n)\nseattle_csv\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFileSystemDataset with 1 csv file\nUsageClass: string\nCheckoutType: string\nMaterialType: string\nCheckoutYear: int64\nCheckoutMonth: int64\nCheckouts: int64\nTitle: string\nISBN: string\nCreator: string\nSubjects: string\nPublisher: string\nPublicationYear: string\n```\n:::\n:::\n\n\n## Your Turn\n\n1.  The first few thousand rows of `ISBN` are blank in the Seattle Checkouts CSV file. Read in the Seattle Checkouts CSV file with `open_dataset()` and ensure the correct data type for `ISBN` is `<string>` instead of the `<null>` interpreted by Arrow.\n2.  Once you have a `Dataset` object with the metadata you are after, count the number of `Checkouts` by `CheckoutYear` and arrange the result by `CheckoutYear`.\n\n‚û°Ô∏è [Data Storage Engineering Exercises Page](03-data-storage-exercises.html)\n\n## 9GB CSV file + arrow + dplyr\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_csv |>\n  group_by(CheckoutYear) |>\n  summarise(sum(Checkouts)) |>\n  collect()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 18 √ó 2\n   CheckoutYear `sum(Checkouts)`\n          <int>            <int>\n 1         2016          9021051\n 2         2022          7001989\n 3         2017          9231648\n 4         2018          9149176\n 5         2019          9199083\n 6         2020          6053717\n 7         2021          7361031\n 8         2005          3798685\n 9         2006          6599318\n10         2007          7126627\n11         2008          8438486\n12         2009          9135167\n13         2010          8608966\n14         2011          8321732\n15         2012          8163046\n16         2013          9057096\n17         2014          9136081\n18         2015          9084179\n```\n:::\n:::\n\n\n## 9GB CSV file + arrow + dplyr\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"5\"}\nseattle_csv |>\n  group_by(CheckoutYear) |>\n  summarise(sum(Checkouts)) |>\n  collect() |>\n  system.time()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   user  system elapsed \n 10.920   1.310  10.616 \n```\n:::\n:::\n\n\n42 million rows -- not bad, but could be faster....\n\n## File Format: Apache Parquet\n\n![](images/apache-parquet.png){.absolute top=\"100\" left=\"200\" width=\"700\"}\n\n::: {style=\"font-size: 60%; margin-top: 450px;\"}\n<https://parquet.apache.org/>\n:::\n\n## Parquet\n\n-   usually smaller than equivalent CSV file\n-   rich type system & stores the data type along with the data\n-   \"column-oriented\" == better performance over CSV's row-by-row\n-   \"row-chunked\" == work on different parts of the file at the same time or skip some chunks all together\n\n::: notes\n-   efficient encodings to keep file size down, and supports file compression, less data to move from disk to memory\n-   CSV has no info about data types, inferred by each parser\n:::\n\n## Parquet Files: \"row-chunked\"\n\n![](images/parquet-chunking.png)\n\n## Parquet Files: \"row-chunked & column-oriented\"\n\n![](images/parquet-columnar.png)\n\n## Writing to Parquet\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_parquet <- here::here(\"data/seattle-library-checkouts-parquet\")\n\nseattle_csv |>\n  write_dataset(path = seattle_parquet,\n                format = \"parquet\")\n```\n:::\n\n\n## Storage: Parquet vs CSV\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfile <- list.files(seattle_parquet)\nfile.size(file.path(seattle_parquet, file)) / 10**9\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4.424267\n```\n:::\n:::\n\n\n<br>\n\nParquet about half the size of the CSV file on-disk üíæ\n\n## Your Turn\n\n1.  Re-run the query counting the number of `Checkouts` by `CheckoutYear` and arranging the result by `CheckoutYear`, this time using the Seattle Checkout data saved to disk as a single, Parquet file. Did you notice a difference in compute time?\n\n‚û°Ô∏è [Data Storage Engineering Exercises Page](03-data-storage-exercises.html)\n\n## 4.5GB Parquet file + arrow + dplyr\n\n\n::: {.cell}\n\n```{.r .cell-code}\nopen_dataset(seattle_parquet, \n             format = \"parquet\") |>\n  group_by(CheckoutYear) |>\n  summarise(sum(Checkouts)) |>\n  arrange(CheckoutYear) |> \n  collect() |>\n  system.time()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   user  system elapsed \n  2.224   0.476   0.629 \n```\n:::\n:::\n\n\n42 million rows -- much better! But could be *even* faster....\n\n## File Storage:<br>Partitioning\n\n<br>\n\n::: columns\n::: {.column width=\"50%\"}\nDividing data into smaller pieces, making it more easily accessible and manageable\n:::\n\n::: {.column width=\"50%\"}\n![](images/partitions.png){.absolute top=\"0\"}\n:::\n:::\n\n::: notes\nalso called multi-files or sometimes shards\n:::\n\n## Poll: Partitioning?\n\nHave you partitioned your data or used partitioned data before today?\n\n-   Yes\n-   No\n-   Not sure, the data engineers sort that out!\n\n## Art & Science of Partitioning\n\n<br>\n\n-   avoid files \\< 20MB and \\> 2GB\n-   avoid \\> 10,000 files (ü§Ø)\n-   partition on variables used in `filter()`\n\n::: notes\n-   guidelines not rules, results vary\n-   experiment\n-   arrow suggests avoid files smaller than 20MB and larger than 2GB\n-   avoid partitions that produce more than 10,000 files\n-   partition by variables that you filter by, allows arrow to only read relevant files\n:::\n\n## Rewriting the Data Again\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_parquet_part <- here::here(\"data/seattle-library-checkouts\")\n\nseattle_csv |>\n  group_by(CheckoutYear) |>\n  write_dataset(path = seattle_parquet_part,\n                format = \"parquet\")\n```\n:::\n\n\n## What Did We \"Engineer\"?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_parquet_part <- here::here(\"data/seattle-library-checkouts\")\n\nsizes <- tibble(\n  files = list.files(seattle_parquet_part, recursive = TRUE),\n  size_GB = file.size(file.path(seattle_parquet_part, files)) / 10**9\n)\n\nsizes\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 18 √ó 2\n   files                            size_GB\n   <chr>                              <dbl>\n 1 CheckoutYear=2005/part-0.parquet   0.115\n 2 CheckoutYear=2006/part-0.parquet   0.172\n 3 CheckoutYear=2007/part-0.parquet   0.186\n 4 CheckoutYear=2008/part-0.parquet   0.204\n 5 CheckoutYear=2009/part-0.parquet   0.224\n 6 CheckoutYear=2010/part-0.parquet   0.233\n 7 CheckoutYear=2011/part-0.parquet   0.250\n 8 CheckoutYear=2012/part-0.parquet   0.261\n 9 CheckoutYear=2013/part-0.parquet   0.282\n10 CheckoutYear=2014/part-0.parquet   0.296\n11 CheckoutYear=2015/part-0.parquet   0.308\n12 CheckoutYear=2016/part-0.parquet   0.315\n13 CheckoutYear=2017/part-0.parquet   0.319\n14 CheckoutYear=2018/part-0.parquet   0.306\n15 CheckoutYear=2019/part-0.parquet   0.303\n16 CheckoutYear=2020/part-0.parquet   0.158\n17 CheckoutYear=2021/part-0.parquet   0.240\n18 CheckoutYear=2022/part-0.parquet   0.252\n```\n:::\n:::\n\n\n## 4.5GB partitioned Parquet files + arrow + dplyr\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_parquet_part <- here::here(\"data/seattle-library-checkouts\")\n\nopen_dataset(seattle_parquet_part,\n             format = \"parquet\") |>\n  group_by(CheckoutYear) |>\n  summarise(sum(Checkouts)) |>\n  collect() |>\n  system.time()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   user  system elapsed \n  1.910   0.401   0.386 \n```\n:::\n:::\n\n\n<br>\n\n42 million rows -- not too shabby!\n\n## Your Turn\n\n1.  Let's write the Seattle Checkout CSV data to a multi-file dataset just one more time! This time, write the data partitioned by `CheckoutType` as Parquet files.\n\n2.  Now compare the compute time between our Parquet data partitioned by `CheckoutYear` and our Parquet data partitioned by `ChekcoutType` with a query of the total number of checkouts in September of 2019. Did you find a difference in compute time?\n\n‚û°Ô∏è [Data Storage Engineering Exercises Page](03-data-storage-exercises.html)\n\n## Partition Design\n\n::: columns\n::: {.column width=\"50%\"}\n-   Partitioning on variables commonly used in `filter()` often faster\n-   Number of partitions also important (Arrow reads the metadata of each file)\n:::\n\n::: {.column width=\"50%\"}\n![](images/partitions.png){.absolute top=\"0\"}\n:::\n:::\n\n## Performance Review: Single CSV\n\nHow long does it take to calculate the number of books checked out in each month of 2021?\n\n<br>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nopen_dataset(\n  sources = here::here(\"data/seattle-library-checkouts.csv\"), \n  format = \"csv\"\n) |> \n  filter(CheckoutYear == 2021, MaterialType == \"BOOK\") |>\n  group_by(CheckoutMonth) |>\n  summarize(TotalCheckouts = sum(Checkouts)) |>\n  arrange(desc(CheckoutMonth)) |>\n  collect() |>\n  system.time()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   user  system elapsed \n 12.140   1.347  11.533 \n```\n:::\n:::\n\n\n## Performance Review: Partitioned Parquet\n\nHow long does it take to calculate the number of books checked out in each month of 2021?\n\n<br>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nopen_dataset(here::here(\"data/seattle-library-checkouts\"),\n             format = \"parquet\") |> \n  filter(CheckoutYear == 2021, MaterialType == \"BOOK\") |>\n  group_by(CheckoutMonth) |>\n  summarize(TotalCheckouts = sum(Checkouts)) |>\n  arrange(desc(CheckoutMonth)) |>\n  collect() |> \n  system.time()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   user  system elapsed \n  0.279   0.040   0.077 \n```\n:::\n:::\n\n\n## Engineering Data Tips for Improved Storage & Performance\n\n<br>\n\n-   consider \"column-oriented\" file formats like Parquet\n-   consider partitioning, experiment to get an appropriate partition design üóÇÔ∏è\n-   watch your schemas üëÄ\n\n## R for Data Science (2e)\n\n::: columns\n::: {.column width=\"50%\"}\n![](images/r4ds-cover.jpg){.absolute top=\"100\" width=\"400\"}\n:::\n\n::: {.column width=\"50%\"}\n<br>\n\n[Chapter 23: Arrow](https://r4ds.hadley.nz/arrow.html)\n\n<br>\n\n<https://r4ds.hadley.nz/>\n:::\n:::\n",
    "supporting": [
      "3_eng_storage_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}