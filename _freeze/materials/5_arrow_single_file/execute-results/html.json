{
  "hash": "a718efc2975cf011bf4c087356f154cf",
  "result": {
    "markdown": "---\nfooter: \"[ðŸ”— posit.io/arrow](https://posit-conf-2023.github.io/arrow)\"\nlogo: \"images/logo.png\"\nexecute:\n  echo: true\nformat:\n  revealjs: \n    theme: default\nengine: knitr\n---\n\n\n# Arrow in R: In-Memory Workflows {#single-file-api}\n\n\n::: {.cell}\n\n:::\n\n\n## arrow ðŸ“¦\n\n![](images/arrow-read-write-updated.png)\n\n## Arrow & Single Files\n\n<br>\n\n`library(arrow)`\n\n-   `read_parquet()`\n-   `read_csv_arrow()`\n-   `read_feather()`\n-   `read_json_arrow()`\n\n**Value**: `tibble` (the default), or an Arrow Table if `as_data_frame = FALSE` --- both *in-memory*\n\n## Your Turn\n\n1.  Read in a single NYC Taxi parquet file using `read_parquet()` as an Arrow Table\n2.  Convert your Arrow Table object to a `data.frame` or a `tibble`\n\n## Read a Parquet File (`tibble`)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(arrow)\n\nparquet_file <- here::here(\"data/nyc-taxi/year=2019/month=9/part-0.parquet\")\n\ntaxi_df <- read_parquet(parquet_file)\ntaxi_df\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6,567,396 Ã— 22\n   vendor_name pickup_datetime     dropoff_datetime    passenger_count\n   <chr>       <dttm>              <dttm>                        <int>\n 1 VTS         2019-09-01 06:14:09 2019-09-01 06:31:52               2\n 2 VTS         2019-09-01 06:36:17 2019-09-01 07:12:44               1\n 3 VTS         2019-09-01 06:29:19 2019-09-01 06:54:13               1\n 4 CMT         2019-09-01 06:33:09 2019-09-01 06:52:14               2\n 5 VTS         2019-09-01 06:57:43 2019-09-01 07:26:21               1\n 6 CMT         2019-09-01 06:59:16 2019-09-01 07:28:12               1\n 7 CMT         2019-09-01 06:20:06 2019-09-01 06:52:19               1\n 8 CMT         2019-09-01 06:27:54 2019-09-01 06:32:56               0\n 9 CMT         2019-09-01 06:35:08 2019-09-01 06:55:51               0\n10 CMT         2019-09-01 06:19:37 2019-09-01 06:30:52               1\n# â„¹ 6,567,386 more rows\n# â„¹ 18 more variables: trip_distance <dbl>, pickup_longitude <dbl>,\n#   pickup_latitude <dbl>, rate_code <chr>, store_and_fwd <chr>,\n#   dropoff_longitude <dbl>, dropoff_latitude <dbl>, payment_type <chr>,\n#   fare_amount <dbl>, extra <dbl>, mta_tax <dbl>, tip_amount <dbl>,\n#   tolls_amount <dbl>, total_amount <dbl>, improvement_surcharge <dbl>,\n#   congestion_surcharge <dbl>, pickup_location_id <int>, â€¦\n```\n:::\n:::\n\n\n## Read a Parquet File (`Table`)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntaxi_table <- read_parquet(parquet_file, as_data_frame = FALSE)\ntaxi_table\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTable\n6567396 rows x 22 columns\n$vendor_name <string>\n$pickup_datetime <timestamp[ms]>\n$dropoff_datetime <timestamp[ms]>\n$passenger_count <int64>\n$trip_distance <double>\n$pickup_longitude <double>\n$pickup_latitude <double>\n$rate_code <string>\n$store_and_fwd <string>\n$dropoff_longitude <double>\n$dropoff_latitude <double>\n$payment_type <string>\n$fare_amount <double>\n$extra <double>\n$mta_tax <double>\n$tip_amount <double>\n$tolls_amount <double>\n$total_amount <double>\n$improvement_surcharge <double>\n$congestion_surcharge <double>\n$pickup_location_id <int64>\n$dropoff_location_id <int64>\n```\n:::\n:::\n\n\n## `tibble` \\<-\\> `Table` \\<-\\> `data.frame`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\n\n#change a df to a table\narrow_table(taxi_df)\n\n#change a table to a df\ntaxi_table |> collect()\n\nas.data.frame(taxi_table)\n\nas_tibble(taxi_table)\n```\n:::\n\n\n<br>\n\n-   `data.frame` & `tibble` are in R memory\n-   `Table` is in Arrow memory\n\n## Data frames\n\n![](images/tabular-structures-r.png)\n\n## Arrow Tables\n\n![](images/tabular-structures-arrow-1.png)\n\n::: notes\nArrow Tables are collections of chunked arrays\n:::\n\n## Table \\| Dataset: A `dplyr` pipeline\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparquet_file |>\n  read_parquet(as_data_frame = FALSE) |>\n  group_by(vendor_name) |>\n  summarise(all_trips = n(),\n            shared_trips = sum(passenger_count > 1, na.rm = TRUE)) |>\n  mutate(pct_shared = shared_trips / all_trips * 100) |>\n  collect()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 Ã— 4\n  vendor_name all_trips shared_trips pct_shared\n  <chr>           <int>        <int>      <dbl>\n1 VTS           4238808      1339478       31.6\n2 CMT           2294473       470344       20.5\n3 <NA>            34115            0        0  \n```\n:::\n:::\n\n\n<br>\n\nFunctions available in Arrow dplyr queries: <https://arrow.apache.org/docs/r/reference/acero.html>\n\n::: notes\nAll the same capabilities as you practiced with Arrow `Dataset`\n:::\n\n## Arrow for Efficient In-Memory Processing\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparquet_file |>\n  read_parquet() |>\n  nrow()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 6567396\n```\n:::\n:::\n\n\n<br>\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"2,4,10\"}\nlibrary(tictoc)\ntic()\nshared_trips <- parquet_file |>\n  read_parquet() |>\n  group_by(vendor_name) |>\n  summarise(all_trips = n(),\n            shared_trips = sum(passenger_count > 1, na.rm = TRUE)) |>\n  mutate(pct_shared = shared_trips / all_trips * 100) |>\n  collect() \ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.734 sec elapsed\n```\n:::\n:::\n\n\n## Arrow for Efficient In-Memory Processing\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparquet_file |>\n  read_parquet(as_data_frame = FALSE) |>\n  nrow()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 6567396\n```\n:::\n:::\n\n\n<br>\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"1,3,9\"}\ntic()\nshared_trips <- parquet_file |>\n  read_parquet(as_data_frame = FALSE) |>\n  group_by(vendor_name) |>\n  summarise(all_trips = n(),\n            shared_trips = sum(passenger_count > 1, na.rm = TRUE)) |>\n  mutate(pct_shared = shared_trips / all_trips * 100) |>\n  collect()\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.377 sec elapsed\n```\n:::\n:::\n\n\n## Read a Parquet File Selectively\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparquet_file |>\n  read_parquet(\n    col_select = c(\"vendor_name\", \"passenger_count\"),\n    as_data_frame = FALSE\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTable\n6567396 rows x 2 columns\n$vendor_name <string>\n$passenger_count <int64>\n```\n:::\n:::\n\n\n## Selective Reads Are Faster\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"1,3,4,12\"}\ntic()\nshared_rides <- parquet_file |>\n  read_parquet(\n    col_select = c(\"vendor_name\", \"passenger_count\"),\n    as_data_frame = FALSE\n  ) |> \n  group_by(vendor_name) |>\n  summarise(all_trips = n(),\n            shared_trips = sum(passenger_count > 1, na.rm = TRUE)) |>\n  mutate(pct_shared = shared_trips / all_trips * 100) |>\n  collect()\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.278 sec elapsed\n```\n:::\n:::\n\n\n## Arrow Table or Dataset?\n\n![](images/2022-09-decision-map.png){.absolute left=\"200\" height=\"550\"}\n\n::: {style=\"font-size: 60%; margin-top: 575px; margin-left: 250px;\"}\n<https://francoismichonneau.net/2022/10/import-big-csv/>\n:::\n\n## Arrow for Improving Those Sluggish Worklows\n\n-   a \"drop-in\" for many `dplyr` workflows (`Table` or `Dataset`)\n-   works when your tabular data get too big for your RAM (`Dataset`)\n-   provides tools for re-engineering data storage for better performance (`arrow::write_dataset()`)\n\n::: notes\nLot's of ways to speed up sluggish workflows e.g. [writing more performant tidyverse code](https://www.tidyverse.org/blog/2023/04/performant-packages/), use other data frame libraries like data.table or polars, use duckDB or other databases, Spark + splarklyr ... However, Arrow offers some attractive features for tackling this challenge, especially for dplyr users.\n:::\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}