{
  "hash": "f2a69d91b4e937e2fe52d6405e226f86",
  "result": {
    "markdown": "---\nfooter: \"[ðŸ”— posit.io/arrow](https://posit-conf-2023.github.io/arrow)\"\nlogo: \"images/logo.png\"\nexecute:\n  echo: true\nformat:\n  revealjs: \n    theme: default\nengine: knitr\n---\n\n\n# Arrow in R: In-Memory Workflows {#single-file-api}\n\n\n::: {.cell}\n\n:::\n\n\n## arrow ðŸ“¦\n\n![](images/arrow-read-write-updated.png)\n\n## Arrow & Single Files\n\n<br>\n\n`library(arrow)`\n\n-   `read_parquet()`\n-   `read_csv_arrow()`\n-   `read_feather()`\n-   `read_json_arrow()`\n\n**Value**: `tibble` (the default), or an Arrow Table if `as_data_frame = FALSE` --- both *in-memory*\n\n## Read a Parquet File (`tibble`)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(arrow)\n\nparquet_file <- here::here(\"data/nyc-taxi/year=2019/month=9/part-0.parquet\")\n\ntaxi_df <- read_parquet(parquet_file)\ntaxi_df\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6,567,396 Ã— 22\n   vendor_name pickup_datetime     dropoff_datetime    passenger_count\n   <chr>       <dttm>              <dttm>                        <int>\n 1 CMT         2019-08-31 18:09:30 2019-08-31 18:15:42               1\n 2 CMT         2019-08-31 18:26:30 2019-08-31 18:44:31               1\n 3 CMT         2019-08-31 18:39:35 2019-08-31 19:15:55               2\n 4 VTS         2019-08-31 18:12:26 2019-08-31 18:15:17               4\n 5 VTS         2019-08-31 18:43:16 2019-08-31 18:53:50               1\n 6 VTS         2019-08-31 18:26:13 2019-08-31 18:45:35               1\n 7 CMT         2019-08-31 18:34:52 2019-08-31 18:42:03               1\n 8 CMT         2019-08-31 18:50:02 2019-08-31 18:58:16               1\n 9 CMT         2019-08-31 18:08:02 2019-08-31 18:14:44               0\n10 VTS         2019-08-31 18:11:38 2019-08-31 18:26:47               1\n# â„¹ 6,567,386 more rows\n# â„¹ 18 more variables: trip_distance <dbl>, pickup_longitude <dbl>,\n#   pickup_latitude <dbl>, rate_code <chr>, store_and_fwd <chr>,\n#   dropoff_longitude <dbl>, dropoff_latitude <dbl>, payment_type <chr>,\n#   fare_amount <dbl>, extra <dbl>, mta_tax <dbl>, tip_amount <dbl>,\n#   tolls_amount <dbl>, total_amount <dbl>, improvement_surcharge <dbl>,\n#   congestion_surcharge <dbl>, pickup_location_id <int>, â€¦\n```\n:::\n:::\n\n\n## Read a Parquet File (`Table`)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntaxi_table <- read_parquet(parquet_file, as_data_frame = FALSE)\ntaxi_table\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTable\n6567396 rows x 22 columns\n$vendor_name <string>\n$pickup_datetime <timestamp[ms]>\n$dropoff_datetime <timestamp[ms]>\n$passenger_count <int64>\n$trip_distance <double>\n$pickup_longitude <double>\n$pickup_latitude <double>\n$rate_code <string>\n$store_and_fwd <string>\n$dropoff_longitude <double>\n$dropoff_latitude <double>\n$payment_type <string>\n$fare_amount <double>\n$extra <double>\n$mta_tax <double>\n$tip_amount <double>\n$tolls_amount <double>\n$total_amount <double>\n$improvement_surcharge <double>\n$congestion_surcharge <double>\n$pickup_location_id <int64>\n$dropoff_location_id <int64>\n```\n:::\n:::\n\n\n## `data.frame` \\<-\\> `Table`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\n\n#change a df to a table\narrow_table(taxi_df)\n\n#change a table to a df\ntaxi_table |> collect()\n\nas.data.frame(taxi_table)\n\nas_tibble(taxi_table)\n```\n:::\n\n\n<br>\n\n-   `data.frame` & `tibble` are in R memory\n-   `Table` is in Arrow memory\n\n## Data frames\n\n![](images/tabular-structures-r.png)\n\n## Arrow Tables\n\n![](images/tabular-structures-arrow-1.png)\n\n::: notes\nArrow Tables are collections of chunked arrays\n:::\n\n## Table \\| Dataset: A `dplyr` pipeline\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparquet_file |>\n  read_parquet(as_data_frame = FALSE) |>\n  group_by(vendor_name) |>\n  summarise(all_trips = n(),\n            shared_trips = sum(passenger_count > 1, na.rm = TRUE)) |>\n  mutate(pct_shared = shared_trips / all_trips * 100) |>\n  collect()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 Ã— 4\n  vendor_name all_trips shared_trips pct_shared\n  <chr>           <int>        <int>      <dbl>\n1 CMT           2294473       470344       20.5\n2 VTS           4238808      1339478       31.6\n3 <NA>            34115            0        0  \n```\n:::\n:::\n\n\n<br>\n\nFunctions available in Arrow dplyr queries: <https://arrow.apache.org/docs/r/reference/acero.html>\n\n::: notes\nAll the same capabilities as you practiced with Arrow `Dataset`\n:::\n\n## Arrow for Efficient In-Memory Processing\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparquet_file |>\n  read_parquet() |>\n  nrow()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 6567396\n```\n:::\n:::\n\n\n<br>\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|2,8\"}\nparquet_file |>\n  read_parquet() |>\n  group_by(vendor_name) |>\n  summarise(all_trips = n(),\n            shared_trips = sum(passenger_count > 1, na.rm = TRUE)) |>\n  mutate(pct_shared = shared_trips / all_trips * 100) |>\n  collect() |>\n  system.time()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   user  system elapsed \n  1.494   0.250   0.564 \n```\n:::\n:::\n\n\n## Arrow for Efficient In-Memory Processing\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparquet_file |>\n  read_parquet(as_data_frame = FALSE) |>\n  nrow()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 6567396\n```\n:::\n:::\n\n\n<br>\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|2,8\"}\nparquet_file |>\n  read_parquet(as_data_frame = FALSE) |>\n  group_by(vendor_name) |>\n  summarise(all_trips = n(),\n            shared_trips = sum(passenger_count > 1, na.rm = TRUE)) |>\n  mutate(pct_shared = shared_trips / all_trips * 100) |>\n  collect() |>\n  system.time()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   user  system elapsed \n  1.359   0.203   0.284 \n```\n:::\n:::\n\n\n## Read a Parquet File Selectively\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparquet_file |>\n  read_parquet(\n    col_select = c(\"vendor_name\", \"passenger_count\"),\n    as_data_frame = FALSE\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTable\n6567396 rows x 2 columns\n$vendor_name <string>\n$passenger_count <int64>\n```\n:::\n:::\n\n\n## Selective Reads Are Faster\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|2,3,11\"}\nparquet_file |>\n  read_parquet(\n    col_select = c(\"vendor_name\", \"passenger_count\"),\n    as_data_frame = FALSE\n  ) |> \n  group_by(vendor_name) |>\n  summarise(all_trips = n(),\n            shared_trips = sum(passenger_count > 1, na.rm = TRUE)) |>\n  mutate(pct_shared = shared_trips / all_trips * 100) |>\n  collect() |>\n  system.time()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   user  system elapsed \n  0.271   0.014   0.143 \n```\n:::\n:::\n\n\n## Arrow Table or Dataset?\n\n![](images/2022-09-decision-map.png){.absolute left=\"200\" height=\"550\"}\n\n::: {style=\"font-size: 60%; margin-top: 575px; margin-left: 250px;\"}\n<https://francoismichonneau.net/2022/10/import-big-csv/>\n:::\n\n## Arrow for Improving Those Sluggish Worklows\n\n-   a \"drop-in\" for many `dplyr` workflows (`Table` or `Dataset`)\n-   works when your tabular data get too big for your RAM (`Dataset`)\n-   provides tools for re-engineering data storage for better performance (`arrow::write_dataset()`)\n\n::: notes\nLot's of ways to speed up sluggish workflows e.g. [writing more performant tidyverse code](https://www.tidyverse.org/blog/2023/04/performant-packages/), use other data frame libraries like data.table or polars, use duckDB or other databases, Spark + splarklyr ... However, Arrow offers some attractive features for tackling this challenge, especially for dplyr users.\n:::\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}